{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to Code a Neural Network from Scratch\n",
    "\n",
    "This notebook demonstrates building a simple neural network from scratch using only **NumPy** for computations and **matplotlib** for visualisations. We will train the network on the **MNIST dataset**, which contains handwritten digits.  \n",
    "\n",
    "**Key topics covered:**  \n",
    "- Dataset preparation  \n",
    "- Activation functions  \n",
    "- Loss functions  \n",
    "- Forward and backward propagation  \n",
    "- Training loop  \n",
    "- Evaluation and visualisation\n",
    "\n",
    "We will mathematically explain each component, including the intuition behind activation functions, loss functions, and the weight update rules used in backpropagation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "334d692f",
   "metadata": {},
   "source": [
    "Structure of a NN\n",
    "\n",
    "Neuron - holds a called the activation; we standardise this\n",
    "Weight - how important a connnecttion is\n",
    "Bias - decides how high the activation needs to be for the neuron to be considered active\n",
    "\n",
    "*Picture of NN*\n",
    "\n",
    "Each circle depicts a neuron and each connection between any 2 neurons has a weight and bias attached to it\n",
    "the first layer is the input layer, this is where all paramaters that you want to take into consideration are inputted; in this case itll be the brightness of each pixel\n",
    "the final layer is the output layer and this will be the networks final decision; in this case it'll be what number itll be looking at\n",
    "Anything between them is called the hidden layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a06a67d",
   "metadata": {},
   "source": [
    "## 1. Setup\n",
    "\n",
    "We import the required libraries:\n",
    "\n",
    "- **NumPy** for matrix operations (essential for vectorised computation of forward and backward passes)  \n",
    "- **matplotlib** for visualising the network’s training progress  \n",
    "- **dataclasses** to define a structured object for storing images and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ccd7e4",
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [],
   "source": [
    "%pip install numpy matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from random import shuffle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a70681c7",
   "metadata": {},
   "source": [
    "## 2. Downloading the Dataset\n",
    "\n",
    "We use the MNIST dataset containing 28x28 grayscale images of handwritten digits. The dataset is downloaded and extracted.  \n",
    "\n",
    "Dataset source: [MNIST Numpy](https://www.kaggle.com/datasets/vikramtiwari/mnist-numpy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "316f2ca7",
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [],
   "source": [
    "!mkdir dataset && curl -L -o dataset/mnist-numpy.zip https://www.kaggle.com/api/v1/datasets/download/vikramtiwari/mnist-numpy && powershell -Command \"Expand-Archive -Force dataset/mnist-numpy.zip dataset; Remove-Item dataset/mnist-numpy.zip\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13dd139b",
   "metadata": {},
   "source": [
    "## 3. Loading and Preprocessing the Dataset\n",
    "\n",
    "Steps performed:  \n",
    "- Load training and test images and labels  \n",
    "- Normalise pixel values to `[0, 1]`  \n",
    "\n",
    "Training data is used to train the weights and biases of the network and the test data is used to test the performance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "    def __init__(self, images: np.ndarray, labels: np.ndarray):\n",
    "        self.images, self.labels = Dataset.adjust(images, labels)\n",
    "\n",
    "    @staticmethod\n",
    "    def adjust(images, labels):\n",
    "        return images / 255, np.eye(10)[labels]\n",
    "        #normalises pixels values from 0-255 (black to white) to 0-1\n",
    "        #makes labels into arrays; eg: a label of 3 becomes [0, 0, 0, 1, 0, 0, 0, 0, 0, 0]\n",
    "\n",
    "def load_data(path):\n",
    "    with np.load(path) as f:\n",
    "        x_train, y_train = f['x_train'], f['y_train'] #training dataset\n",
    "        x_test, y_test = f['x_test'], f['y_test'] #test dataset\n",
    "        return Dataset(x_train, y_train), Dataset(x_test, y_test)\n",
    "    \n",
    "training, test = load_data('./dataset/mnist.npz')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2261d2ea",
   "metadata": {},
   "source": [
    "## 4. Hyperparameters\n",
    "\n",
    "Define essential hyperparameters for training:  \n",
    "\n",
    "- **Learning rate**: controls the step size in gradient descent  \n",
    "- **Epochs**: number of times the entire dataset is passed through the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn_rate = 0.012\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3864824a",
   "metadata": {},
   "source": [
    "## 5. Activation Functions\n",
    "\n",
    "Activation functions introduce non-linearity to the network. We implement:  \n",
    "\n",
    "- **Sigmoid**  \n",
    "- **ReLU**  \n",
    "- **Tanh**  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b978502",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _sigmoid(x, derivative = False):\n",
    "    if derivative:\n",
    "        s = _sigmoid(x) \n",
    "        return s * (1 - s)\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def _relu(x, derivative = False):\n",
    "    if derivative:\n",
    "        return (x > 0).astype(float)\n",
    "    return x * (x > 0)\n",
    "\n",
    "def _tanh(x, derivative = False):\n",
    "    if derivative:\n",
    "        return 1 - np.tanh(x) ** 2\n",
    "    return np.tanh(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "830b145c",
   "metadata": {},
   "source": [
    "## 6. Loss / Cost Functions\n",
    "\n",
    "Loss functions measure the network’s prediction error. We implement:  \n",
    "\n",
    "- **Mean Squared Error** (good for regression tasks)  \n",
    "- **Cross-Entropy Loss** (suitable for classification)  \n",
    "\n",
    "We select **cross-entropy loss** for MNIST classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b5d46a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _mean_squared_error(x, y): #better for regression\n",
    "    return np.mean((x - y) ** 2)\n",
    "\n",
    "def _cross_entropy_loss(x, y):\n",
    "    x = np.clip(x, 1e-12, 1.0) # safety in case x = 0 \n",
    "    return -np.mean(np.sum(y * np.log(x), axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4daea48",
   "metadata": {},
   "source": [
    "And now lets choose each function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a0bf54a",
   "metadata": {},
   "outputs": [],
   "source": [
    "activation_h = _sigmoid\n",
    "activation_o = _sigmoid\n",
    "cost = _cross_entropy_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c6b75d5",
   "metadata": {},
   "source": [
    "## 7. Network Architecture\n",
    "\n",
    "A simple fully connected neural network:  \n",
    "\n",
    "- Input layer: 784 neurons (28x28 flattened image)  \n",
    "- Hidden layer: 40 neurons  \n",
    "- Output layer: 10 neurons (digits 0–9)  \n",
    "\n",
    "Weights and biases are initialised randomly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e099175f",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_layer_nodes = 784\n",
    "hidden_layer_nodes = 40\n",
    "output_layer_nodes = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4945e7af",
   "metadata": {},
   "source": [
    "w = weights, b = bias, i = input, h = hidden, o = output\n",
    "e.g. w_i_h = weights from input layer to hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bafdc0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "w_i_h = np.random.uniform(-0.5, 0.5, (hidden_layer_nodes, input_layer_nodes)) #weights for input layer to hidden layer\n",
    "w_h_o = np.random.uniform(-0.5, 0.5, (output_layer_nodes, hidden_layer_nodes)) #weights for hidden layer to output layer\n",
    "b_i_h = np.zeros((hidden_layer_nodes, 1)) #bias for input layer to hidden layer\n",
    "b_h_o = np.zeros((output_layer_nodes, 1)) #bias for output layer to output layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe6c5ed",
   "metadata": {},
   "source": [
    "## 8. Forward and Backward Propagation\n",
    "\n",
    "- **Forward propagation**: computes the network output for a given input  \n",
    "- **Backward propagation**: updates weights and biases using gradient descent  \n",
    "\n",
    "These functions implement the core of training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d5b2cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(image, w_i_h, w_h_o, b_i_h, b_h_o):\n",
    "    #input -> hidden\n",
    "    hidden = activation_h(b_i_h + w_i_h @ image)\n",
    "    #hidden -> output\n",
    "    output = activation_o(b_h_o + w_h_o @ hidden)\n",
    "    return output, hidden\n",
    "\n",
    "def back(difference, hidden, image, w_i_h, w_h_o, b_i_h, b_h_o, learn_rate): #difference = delta_o\n",
    "    delta_h = w_h_o.T @ difference * activation_h(b_i_h + w_i_h @ image, True)\n",
    "    #output -> hidden (cost function derivative)\n",
    "    w_h_o += -learn_rate * difference @ hidden.T\n",
    "    b_h_o += -learn_rate * difference\n",
    "    #hidden -> input (activation function derivative)\n",
    "    w_i_h += -learn_rate * delta_h @ image.T\n",
    "    b_i_h += -learn_rate * delta_h\n",
    "    return w_i_h, w_h_o, b_i_h, b_h_o"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "195867df",
   "metadata": {},
   "source": [
    "## 9. Training the Network\n",
    "\n",
    "Training loop steps:  \n",
    "1. Shuffle dataset each epoch  \n",
    "2. Perform forward and backward propagation for each sample  \n",
    "3. Calculate accuracy and loss  \n",
    "4. Track errors and accuracies for plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c08c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataset, epochs, learn_rate, w_i_h, w_h_o, b_i_h, b_h_o):\n",
    "    epoch_errors = []\n",
    "    epoch_accuracies = []\n",
    "    n_samples = dataset.images.shape[0]\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        correct = 0\n",
    "        total_error = 0\n",
    "        dataset_list = list(zip(dataset.images, dataset.labels))\n",
    "        shuffle(dataset_list)\n",
    "\n",
    "        for image, label in dataset_list:\n",
    "            image = image.reshape(784, 1)\n",
    "            label = label.reshape(10, 1)\n",
    "\n",
    "            output, hidden = forward(image, w_i_h, w_h_o, b_i_h, b_h_o)\n",
    "\n",
    "            error = cost(output, label)\n",
    "            total_error += error\n",
    "\n",
    "            correct += int(np.argmax(output) == np.argmax(label))\n",
    "\n",
    "            w_i_h, w_h_o, b_i_h, b_h_o = back(output - label, hidden, image, w_i_h, w_h_o, b_i_h, b_h_o, learn_rate)\n",
    "\n",
    "        avg_error = total_error / n_samples\n",
    "        accuracy = correct / n_samples * 100\n",
    "        epoch_errors.append(avg_error)\n",
    "        epoch_accuracies.append(accuracy)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Accuracy: {accuracy:.2f}%, Error: {avg_error:.6f}\")\n",
    "\n",
    "    return w_i_h, w_h_o, b_i_h, b_h_o, epoch_errors, epoch_accuracies\n",
    "\n",
    "def plot(errors, accuracies):\n",
    "    fig, ax1 = plt.subplots()\n",
    "\n",
    "    color = 'tab:red'\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Average Error', color=color)\n",
    "    ax1.plot(range(1, epochs + 1), errors, marker='o', color=color)\n",
    "    ax1.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "    ax2 = ax1.twinx() \n",
    "    color = 'tab:blue'\n",
    "    ax2.set_ylabel('Accuracy (%)', color=color)\n",
    "    ax2.plot(range(1, epochs + 1), accuracies, marker='x', color=color)\n",
    "    ax2.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "    plt.title(\"Training Error and Accuracy Over Epochs\")\n",
    "    fig.tight_layout()\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ecf036",
   "metadata": {},
   "source": [
    "## 10. Training Results\n",
    "\n",
    "We train the network and plot:  \n",
    "\n",
    "- **Training loss over epochs**  \n",
    "- **Training accuracy over epochs**  \n",
    "\n",
    "This helps us visually confirm learning progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d6f905",
   "metadata": {},
   "outputs": [],
   "source": [
    "w_i_h, w_h_o, b_i_h, b_h_o, errors, accuracies = train(training, epochs, learn_rate, w_i_h, w_h_o, b_i_h, b_h_o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc69f4e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(errors, accuracies)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "807db9d0",
   "metadata": {},
   "source": [
    "## 11. Testing the Network\n",
    "\n",
    "We evaluate the trained network on unseen test data and compute accuracy.  \n",
    "\n",
    "- Count how many predictions match true labels  \n",
    "- Calculate overall test accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c624330",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(test_dataset):\n",
    "    count = 0\n",
    "    misclassified_indices = []\n",
    "    for idx, (image, label) in enumerate(zip(test_dataset.images, test_dataset.labels)):\n",
    "        image = image.reshape(784, 1)\n",
    "        output, _ = forward(image, w_i_h, w_h_o, b_i_h, b_h_o)\n",
    "        \n",
    "        if np.argmax(output) != np.argmax(label): misclassified_indices.append(idx)\n",
    "        else: count += 1\n",
    "    print(f\"{count}/{idx + 1}\\t{count/(idx + 1) : .2f}%\")\n",
    "    return misclassified_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f08171",
   "metadata": {},
   "outputs": [],
   "source": [
    "misclassified = test(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "791b3d2c",
   "metadata": {},
   "source": [
    "## 12. Visualising Predictions\n",
    "\n",
    "We inspect individual test samples:  \n",
    "\n",
    "- Display the image  \n",
    "- Show the network’s predicted label and confidence for each class  \n",
    "- Helps interpret network behaviour on specific digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f563a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from ipywidgets import interact\n",
    "\n",
    "def show_test_sample(test_dataset, idx, w_i_h, w_h_o, b_i_h, b_h_o):\n",
    "    image = test_dataset.images[idx].reshape(784, 1)\n",
    "    true_label = np.argmax(test_dataset.labels[idx])\n",
    "\n",
    "    output, _ = forward(image, w_i_h, w_h_o, b_i_h, b_h_o)\n",
    "    output = output.flatten()\n",
    "    pred_label = int(np.argmax(output))\n",
    "\n",
    "    fig, (ax_img, ax_bar) = plt.subplots(1, 2, figsize=(10, 4))\n",
    "    ax_img.imshow(test_dataset.images[idx].reshape(28, 28), cmap=\"gray\")\n",
    "    ax_img.set_title(f\"True: {true_label}, Pred: {pred_label}\")\n",
    "    ax_img.axis(\"off\")\n",
    "\n",
    "    ax_bar.bar(range(10), output)\n",
    "    ax_bar.set_xticks(range(10))\n",
    "    ax_bar.set_ylim(0, 1)\n",
    "    ax_bar.set_xlabel(\"Digit\")\n",
    "    ax_bar.set_ylabel(\"Confidence\")\n",
    "    ax_bar.set_title(\"Network Output Confidence\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "@interact(idx=(0, len(misclassified)-1))\n",
    "def interactive_view(idx=0):\n",
    "    show_test_sample(test, misclassified[idx], w_i_h, w_h_o, b_i_h, b_h_o)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf13796",
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint\n",
    "show_test_sample(test, randint(0, test.images.shape[0] - 1), w_i_h, w_h_o, b_i_h, b_h_o)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
